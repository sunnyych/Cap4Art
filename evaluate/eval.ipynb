{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTt6S03fjsgaCHJoNRyfHP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# If your annotation_file/result_file live in Drive, you can mount:\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","FOLDERNAME = \"cs231n/project/\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KpaOSmeNo9D-","executionInfo":{"status":"ok","timestamp":1748824164567,"user_tz":420,"elapsed":706,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}},"outputId":"83c2a729-59a9-4fc1-822c-2ed89edb46dc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**Process model-customized output captions_eval.json to generate COCO-format annotation json files**\n","\n","Transform your custom annotation input.\n","```\n","[\n","  {\n","    \"image_path\": \"/content/drive/My Drive/cs231n/project/test/ALandscapewithaRuinedCastleandaChurch.png\",\n","    \"ground_truth\": [\n","      \"On a pool are three swans.\"\n","    ],\n","    \"baseline_caption\": \"a painting of a river with a castle in the background\",\n","    \"finetuned_caption\": \"in the foreground, with a view of the countryside beyond, is an oil - on - canvas painting by vincent van gogh\"\n","  }\n","]\n","```\n","\n","to\n"," 1. a COCO-style annotation JSON file(\"images\" + annotations\" sections\")\n"," 2. Two or more sepearte \"result\" JSON files in the format that COCO's evalution code expects:\n","  *   baseline model's captions\n","  *   fine-tuned model's captions"],"metadata":{"id":"X82lK3Jh08cT"}},{"cell_type":"code","source":["import json\n","import os\n","\n","def convert_custom_to_coco(\n","    input_path: str,\n","    groundtruth_annotation_path: str,\n","    baseline_results_path: str,\n","    finetuned_results_path: str\n","):\n","    \"\"\"\n","    Read your custom-format JSON and produce:\n","      1) a COCO-style ground-truth annotation JSON (written to groundtruth_annotation_path)\n","      2) a \"baseline\" result JSON (written to baseline_results_path)\n","      3) a \"finetuned\" result JSON (written to finetuned_results_path)\n","\n","    Args:\n","        input_path (str):\n","            Path to your original JSON. It should look like:\n","            [\n","                {\n","                    \"image_path\": \"/content/drive/My Drive/cs231n/project/test/ALandscapewithaRuinedCastleandaChurch.png\",\n","                    \"ground_truth\": [\n","                        \"On a pool are three swans.\"\n","                    ],\n","                    \"baseline_caption\": \"a painting of a river with a castle in the background\",\n","                    \"finetuned_caption\": \"in the foreground, with a view of the countryside beyond, is an oil-on-canvas painting by vincent van gogh\"\n","                },\n","                {\n","                    \"image_path\": \"/content/drive/My Drive/cs231n/project/test/AloneintheWorld_Bouguereau_.png\",\n","                    \"ground_truth\": [\n","                        \"This indicates that the girl is standing on the Pont de Solférino.\"\n","                    ],\n","                    \"baseline_caption\": \"a painting of a young girl holding a violin\",\n","                    \"finetuned_caption\": \"the painting depicts a young woman holding a violin in front of a cityscape\"\n","                },\n","                {\n","                    \"image_path\": \"/content/drive/My Drive/cs231n/project/test/AmericanProgress.png\",\n","                    \"ground_truth\": [\n","                        \"Progress lays a telegraph wire with one hand and carries a school book in the other.\",\n","                        \"As she moves westward, indigenous people and a herd of buffalo are seen fleeing her and the settlers.\"\n","                    ],\n","                    \"baseline_caption\": \"a painting of a woman flying in the air\",\n","                    \"finetuned_caption\": \"in the center of the painting, the woman in the foreground is the subject of the artist's work.\"\n","                },\n","                …\n","            ]\n","\n","        groundtruth_annotation_path (str):\n","            Where to write out the COCO-style ground-truth annotation JSON, e.g.\n","            \"/content/drive/MyDrive/cs231n/project/test/groundtruth_annotations.json\".\n","\n","        baseline_results_path (str):\n","            Where to write the baseline model’s results JSON, e.g.\n","            \"/content/drive/MyDrive/cs231n/project/test/baseline_results.json\".\n","\n","        finetuned_results_path (str):\n","            Where to write the finetuned model’s results JSON, e.g.\n","            \"/content/drive/MyDrive/cs231n/project/test/finetuned_results.json\".\n","    \"\"\"\n","\n","    # 1) Load your custom-format JSON\n","    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","\n","    coco_root = {\n","        \"images\": [],\n","        \"annotations\": []\n","    }\n","\n","    baseline_results = []\n","    finetuned_results = []\n","\n","    annotation_id = 0\n","\n","    # Assign a unique integer ID to each image (start from 1)\n","    for idx, entry in enumerate(data, start=1):\n","        image_id = idx\n","        image_path = entry[\"image_path\"]\n","        file_name = os.path.basename(image_path)\n","\n","        # Add this image to the \"images\" list\n","        coco_root[\"images\"].append({\n","            \"id\": image_id,\n","            \"file_name\": file_name\n","        })\n","\n","        # Add one annotation object per ground-truth caption\n","        for gt_caption in entry.get(\"ground_truth\", []):\n","            annotation_id += 1\n","            coco_root[\"annotations\"].append({\n","                \"id\": annotation_id,\n","                \"image_id\": image_id,\n","                \"caption\": gt_caption\n","            })\n","\n","        # Build the baseline result entry\n","        baseline_results.append({\n","            \"image_id\": image_id,\n","            \"caption\": entry[\"baseline_caption\"]\n","        })\n","\n","        # Build the finetuned result entry\n","        finetuned_results.append({\n","            \"image_id\": image_id,\n","            \"caption\": entry[\"finetuned_caption\"]\n","        })\n","\n","    # 2) Write out the COCO-style ground-truth JSON\n","    with open(groundtruth_annotation_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(coco_root, f, ensure_ascii=False, indent=2)\n","\n","    # 3) Write out the baseline results JSON\n","    with open(baseline_results_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(baseline_results, f, ensure_ascii=False, indent=2)\n","\n","    # 4) Write out the finetuned results JSON\n","    with open(finetuned_results_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(finetuned_results, f, ensure_ascii=False, indent=2)\n","\n","    print(f\"Wrote ground-truth annotations → {groundtruth_annotation_path}\")\n","    print(f\"Wrote baseline results            → {baseline_results_path}\")\n","    print(f\"Wrote finetuned results           → {finetuned_results_path}\")"],"metadata":{"id":"T-09Ilna07ve","executionInfo":{"status":"ok","timestamp":1748824164580,"user_tz":420,"elapsed":14,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["\n","# ──────────────────────────────────────────────────────────────────────────────\n","# Example of how to call the converter in Colab:\n","#\n","# Suppose your original custom JSON is:\n","input_json = \"/content/drive/MyDrive/cs231n/project/eval_data/captions_eval.json\"\n","\n","# Define output paths (all on Drive, for instance):\n","groundtruth_annotation_path = \"/content/drive/MyDrive/cs231n/project/eval_data/groundtruth_annotations.json\"\n","baseline_results_path       = \"/content/drive/MyDrive/cs231n/project/eval_data/baseline_results.json\"\n","finetuned_results_path      = \"/content/drive/MyDrive/cs231n/project/eval_data/finetuned_results.json\"\n","\n","convert_custom_to_coco(\n","    input_path=input_json,\n","    groundtruth_annotation_path=groundtruth_annotation_path,\n","    baseline_results_path=baseline_results_path,\n","    finetuned_results_path=finetuned_results_path\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZRSIJ_42VLX","executionInfo":{"status":"ok","timestamp":1748824164590,"user_tz":420,"elapsed":9,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}},"outputId":"d35c4462-c4a2-43b9-ac47-61a1410bbc2d"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote ground-truth annotations → /content/drive/MyDrive/cs231n/project/eval_data/groundtruth_annotations.json\n","Wrote baseline results            → /content/drive/MyDrive/cs231n/project/eval_data/baseline_results.json\n","Wrote finetuned results           → /content/drive/MyDrive/cs231n/project/eval_data/finetuned_results.json\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/cs231n/project/\n","!git clone https://github.com/salaniz/pycocoevalcap.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGyk6yspuMw-","executionInfo":{"status":"ok","timestamp":1748824164735,"user_tz":420,"elapsed":143,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}},"outputId":"753642c6-f008-4629-d9c3-18590e861fe5"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1wr3oM_e1S6oqX5xOKf31n-KmlL40G0MY/cs231n/project\n","fatal: destination path 'pycocoevalcap' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/cs231n/project/pycocoevalcap\n","!pip install -e .\n","!pip install pycocotools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkI3_pB6un_z","executionInfo":{"status":"ok","timestamp":1748824175972,"user_tz":420,"elapsed":11235,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}},"outputId":"11a77bad-f6d9-455d-a420-78498b61b130"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1wr3oM_e1S6oqX5xOKf31n-KmlL40G0MY/cs231n/project/pycocoevalcap\n","Obtaining file:///content/drive/.shortcut-targets-by-id/1wr3oM_e1S6oqX5xOKf31n-KmlL40G0MY/cs231n/project/pycocoevalcap\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.8)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.17.0)\n","Installing collected packages: pycocoevalcap\n","  Attempting uninstall: pycocoevalcap\n","    Found existing installation: pycocoevalcap 1.2\n","    Uninstalling pycocoevalcap-1.2:\n","      Successfully uninstalled pycocoevalcap-1.2\n","  Running setup.py develop for pycocoevalcap\n","Successfully installed pycocoevalcap-1.2\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/cs231n/project/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1q-O1wcvH9Z","executionInfo":{"status":"ok","timestamp":1748824175979,"user_tz":420,"elapsed":9,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}},"outputId":"d5bbe1d4-8c35-4319-d99d-3d809e864c5f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1wr3oM_e1S6oqX5xOKf31n-KmlL40G0MY/cs231n/project\n"]}]},{"cell_type":"markdown","source":["Assume you have two JSON files:\n","- human_annoation.json(COCO-format ground-truth annotations)\n","\n","- caption_results.json(your generated captions in COCO result format)\n","\n","You can place/upload these files in your drive /content."],"metadata":{"id":"NZmeidPBvsU6"}},{"cell_type":"code","source":["from pycocotools.coco import COCO\n","from pycocoevalcap.eval import COCOEvalCap\n","\n","def evaluate_captions(annotation_file, result_file):\n","    \"\"\"\n","    Evaluate generated captions against COCO ground-truth annotations.\n","\n","    Args:\n","        annotation_file (str): Path to the COCO-format annotation JSON.\n","        result_file (str): Path to the JSON file containing generated captions.\n","                           Example format:\n","                           [\n","                             {\"image_id\": 42, \"caption\": \"a man riding a horse\"},\n","                             {\"image_id\": 73, \"caption\": \"two dogs playing in the park\"},\n","                             ...\n","                           ]\n","\n","    Returns:\n","        dict: A dictionary of evaluation metrics (BLEU, METEOR, ROUGE_L, CIDEr, SPICE, etc.).\n","    \"\"\"\n","    # 1) Load COCO ground-truth annotations\n","    coco = COCO(annotation_file)\n","\n","    # 2) Load the generated captions (must match COCO result format)\n","    cocoRes = coco.loadRes(result_file)\n","\n","    # 3) Create the COCO evaluator\n","    cocoEval = COCOEvalCap(coco, cocoRes)\n","\n","    # 4) Evaluate only on the images present in your result file\n","    cocoEval.params['image_id'] = cocoRes.getImgIds()\n","\n","    # 5) Run evaluation\n","    cocoEval.evaluate()\n","\n","    # 6) Return the computed metrics as a dictionary\n","    return cocoEval.eval"],"metadata":{"id":"P9k97VaQqIj8","executionInfo":{"status":"ok","timestamp":1748824176015,"user_tz":420,"elapsed":27,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["COCO’s caption evaluator (COCOEvalCap) by default computes these seven scores for each set of predicted captions:\n","\n","1. Bleu_1, Bleu_2, Bleu_3, Bleu_4\n","\n","  BLEU (BiLingual Evaluation Understudy) n-gram precision scores, where Bleu_1 uses unigrams, Bleu_2 uses bigrams, and so on up to Bleu_4 (4-grams). Higher n means a stricter match.\n","\n","  See the original BLEU paper for details: https://www.aclweb.org/anthology/P02-1040.pdf\n","\n","  And a summary in the COCO-caption repo:\n","https://github.com/salaniz/pycocoevalcap/blob/master/pycocoevalcap/bleu/bleu.py\n","\n","2. METEOR\n","\n","Stands for “Metric for Evaluation of Translation with Explicit ORdering.” It aligns unigrams by exact, stem, synonym, and paraphrase matches, then computes a precision/recall harmonic mean with a penalty for word-order differences.\n","\n","More info: https://www.cs.cmu.edu/~alavie/METEOR/\n","\n","3. ROUGE_L\n","\n","ROUGE-L measures the longest common subsequence (LCS) between candidate and reference captions, capturing sentence-level structure.\n","\n","Details: https://aclanthology.org/W04-1013.pdf\n","\n","4. CIDEr\n","\n","  “Consensus‐Based Image Description Evaluation.” It weights n-grams by how frequently they appear in the reference corpus (TF-IDF style), then computes a cosine similarity between candidate and references. This tends to reward captions that match the consensus of human annotations.\n","\n","  Read the CIDEr paper: https://arxiv.org/abs/1411.5726\n","\n","5. SPICE\n","\n","  “Semantic Propositional Image Caption Evaluation.” Instead of n-gram overlap, SPICE parses both candidate and reference captions into scene graphs (objects, attributes, relations) and computes an F-score over those. It tends to correlate better with human judgments of semantics.\n","\n","  See the SPICE paper: https://arxiv.org/abs/1604.08889\n"],"metadata":{"id":"6kKxt3XJ5oNP"}},{"cell_type":"code","source":["#Run evaluations and print raw metric dictionaries\n","\n","import pandas as pd\n","from IPython.display import display\n","\n","# Paths to our JSON files\n","groundtruth_ann = \"/content/drive/MyDrive/cs231n/project/eval_data/groundtruth_annotations.json\"\n","baseline_res    = \"/content/drive/MyDrive/cs231n/project/eval_data/baseline_results.json\"\n","finetuned_res   = \"/content/drive/MyDrive/cs231n/project/eval_data/finetuned_results.json\"\n","\n","# ----- Baseline Model -----\n","print(\"Baseline model evaluation:\")\n","baseline_metrics = evaluate_captions(groundtruth_ann, baseline_res)\n","print(baseline_metrics)\n","\n","# ----- Finetuned Model -----\n","print(\"\\nFinetuned model evaluation:\")\n","finetuned_metrics = evaluate_captions(groundtruth_ann, finetuned_res)\n","print(finetuned_metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0lZ9ZvvxsE0","executionInfo":{"status":"ok","timestamp":1748824225578,"user_tz":420,"elapsed":49562,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}},"outputId":"7ad734d3-de55-4306-ef51-15768c57b0aa"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline model evaluation:\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","tokenization...\n","setting up scorers...\n","computing Bleu score...\n","{'testlen': 803, 'reflen': 2007, 'guess': [803, 716, 629, 542], 'correct': [244, 30, 5, 1]}\n","ratio: 0.4000996512205281\n","Bleu_1: 0.068\n","Bleu_2: 0.025\n","Bleu_3: 0.010\n","Bleu_4: 0.005\n","computing METEOR score...\n","METEOR: 0.041\n","computing Rouge score...\n","ROUGE_L: 0.131\n","computing CIDEr score...\n","CIDEr: 0.047\n","computing SPICE score...\n","SPICE: 0.057\n","{'Bleu_1': 0.067842677280321, 'Bleu_2': 0.025192423813897684, 'Bleu_3': 0.010404694216117031, 'Bleu_4': 0.004641189954504011, 'METEOR': 0.041186415640672124, 'ROUGE_L': np.float64(0.1306372175038461), 'CIDEr': np.float64(0.046974308715503926), 'SPICE': np.float64(0.05737535715845713)}\n","\n","Finetuned model evaluation:\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","tokenization...\n","setting up scorers...\n","computing Bleu score...\n","{'testlen': 1717, 'reflen': 2057, 'guess': [1717, 1630, 1543, 1456], 'correct': [461, 70, 11, 3]}\n","ratio: 0.8347107438012471\n","Bleu_1: 0.220\n","Bleu_2: 0.088\n","Bleu_3: 0.036\n","Bleu_4: 0.017\n","computing METEOR score...\n","METEOR: 0.069\n","computing Rouge score...\n","ROUGE_L: 0.187\n","computing CIDEr score...\n","CIDEr: 0.123\n","computing SPICE score...\n","SPICE: 0.051\n","{'Bleu_1': 0.2202580159868429, 'Bleu_2': 0.088088989001533, 'Bleu_3': 0.03566895350122068, 'Bleu_4': 0.016642095844817305, 'METEOR': 0.06919026542511927, 'ROUGE_L': np.float64(0.1868143120967541), 'CIDEr': np.float64(0.12255295163102933), 'SPICE': np.float64(0.05068074164170498)}\n"]}]},{"cell_type":"code","source":["# Baseline metrics table\n","baseline_df = pd.DataFrame.from_dict(baseline_metrics, orient='index', columns=['Score'])\n","baseline_df.index.name = 'Metric'\n","print(\"Baseline Model Metrics:\")\n","display(baseline_df)\n","\n","# Finetuned metrics table\n","finetuned_df = pd.DataFrame.from_dict(finetuned_metrics, orient='index', columns=['Score'])\n","finetuned_df.index.name = 'Metric'\n","print(\"\\nFinetuned Model Metrics:\")\n","display(finetuned_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":698},"id":"WzPiswl47QtD","executionInfo":{"status":"ok","timestamp":1748824225640,"user_tz":420,"elapsed":59,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}},"outputId":"1d99105e-d396-4773-ec17-100461726540"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline Model Metrics:\n"]},{"output_type":"display_data","data":{"text/plain":["            Score\n","Metric           \n","Bleu_1   0.067843\n","Bleu_2   0.025192\n","Bleu_3   0.010405\n","Bleu_4   0.004641\n","METEOR   0.041186\n","ROUGE_L  0.130637\n","CIDEr    0.046974\n","SPICE    0.057375"],"text/html":["\n","  <div id=\"df-cc8f9e39-b23d-4eaf-a355-f08e25596161\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Score</th>\n","    </tr>\n","    <tr>\n","      <th>Metric</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Bleu_1</th>\n","      <td>0.067843</td>\n","    </tr>\n","    <tr>\n","      <th>Bleu_2</th>\n","      <td>0.025192</td>\n","    </tr>\n","    <tr>\n","      <th>Bleu_3</th>\n","      <td>0.010405</td>\n","    </tr>\n","    <tr>\n","      <th>Bleu_4</th>\n","      <td>0.004641</td>\n","    </tr>\n","    <tr>\n","      <th>METEOR</th>\n","      <td>0.041186</td>\n","    </tr>\n","    <tr>\n","      <th>ROUGE_L</th>\n","      <td>0.130637</td>\n","    </tr>\n","    <tr>\n","      <th>CIDEr</th>\n","      <td>0.046974</td>\n","    </tr>\n","    <tr>\n","      <th>SPICE</th>\n","      <td>0.057375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc8f9e39-b23d-4eaf-a355-f08e25596161')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cc8f9e39-b23d-4eaf-a355-f08e25596161 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cc8f9e39-b23d-4eaf-a355-f08e25596161');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f7ea4ee0-288e-409e-b54a-13bd0687de75\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f7ea4ee0-288e-409e-b54a-13bd0687de75')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f7ea4ee0-288e-409e-b54a-13bd0687de75 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_8228e450-54bb-4f6e-b6ff-5a4d7b51a5ac\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('baseline_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8228e450-54bb-4f6e-b6ff-5a4d7b51a5ac button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('baseline_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"baseline_df","summary":"{\n  \"name\": \"baseline_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Bleu_2\",\n          \"ROUGE_L\",\n          \"Bleu_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.039949229796219704,\n        \"min\": 0.004641189954504011,\n        \"max\": 0.1306372175038461,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.025192423813897684,\n          0.1306372175038461,\n          0.067842677280321\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Finetuned Model Metrics:\n"]},{"output_type":"display_data","data":{"text/plain":["            Score\n","Metric           \n","Bleu_1   0.220258\n","Bleu_2   0.088089\n","Bleu_3   0.035669\n","Bleu_4   0.016642\n","METEOR   0.069190\n","ROUGE_L  0.186814\n","CIDEr    0.122553\n","SPICE    0.050681"],"text/html":["\n","  <div id=\"df-544c7fb3-070f-447b-afc3-cfe129d50b28\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Score</th>\n","    </tr>\n","    <tr>\n","      <th>Metric</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Bleu_1</th>\n","      <td>0.220258</td>\n","    </tr>\n","    <tr>\n","      <th>Bleu_2</th>\n","      <td>0.088089</td>\n","    </tr>\n","    <tr>\n","      <th>Bleu_3</th>\n","      <td>0.035669</td>\n","    </tr>\n","    <tr>\n","      <th>Bleu_4</th>\n","      <td>0.016642</td>\n","    </tr>\n","    <tr>\n","      <th>METEOR</th>\n","      <td>0.069190</td>\n","    </tr>\n","    <tr>\n","      <th>ROUGE_L</th>\n","      <td>0.186814</td>\n","    </tr>\n","    <tr>\n","      <th>CIDEr</th>\n","      <td>0.122553</td>\n","    </tr>\n","    <tr>\n","      <th>SPICE</th>\n","      <td>0.050681</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-544c7fb3-070f-447b-afc3-cfe129d50b28')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-544c7fb3-070f-447b-afc3-cfe129d50b28 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-544c7fb3-070f-447b-afc3-cfe129d50b28');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-e00b9d28-c0ee-4eec-9f03-40eb357c479a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e00b9d28-c0ee-4eec-9f03-40eb357c479a')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-e00b9d28-c0ee-4eec-9f03-40eb357c479a button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_79ec3ba0-4851-493c-a93a-b1c22eb9e2a8\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('finetuned_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_79ec3ba0-4851-493c-a93a-b1c22eb9e2a8 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('finetuned_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"finetuned_df","summary":"{\n  \"name\": \"finetuned_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Bleu_2\",\n          \"ROUGE_L\",\n          \"Bleu_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07280611747435181,\n        \"min\": 0.016642095844817305,\n        \"max\": 0.2202580159868429,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.088088989001533,\n          0.1868143120967541,\n          0.2202580159868429\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","from IPython.display import display\n","\n","# Original JSON paths\n","orig_gt_path        = \"/content/drive/MyDrive/cs231n/project/eval_data/groundtruth_annotations.json\"\n","orig_baseline_path  = \"/content/drive/MyDrive/cs231n/project/eval_data/baseline_results.json\"\n","orig_finetuned_path = \"/content/drive/MyDrive/cs231n/project/eval_data/finetuned_results.json\"\n","\n","# Paths for filtered outputs\n","filtered_gt_path        = \"/content/drive/MyDrive/cs231n/project/eval_data/gt_at_least5.json\"\n","filtered_baseline_path  = \"/content/drive/MyDrive/cs231n/project/eval_data/baseline_at_least5.json\"\n","filtered_finetuned_path = \"/content/drive/MyDrive/cs231n/project/eval_data/finetuned_at_least5.json\"\n","\n","# 1) Load the original ground-truth annotations\n","with open(orig_gt_path, \"r\", encoding=\"utf-8\") as f:\n","    coco = json.load(f)\n","\n","# 2) Count how many captions each image_id has\n","caption_counts = {}\n","for ann in coco[\"annotations\"]:\n","    img_id = ann[\"image_id\"]\n","    caption_counts[img_id] = caption_counts.get(img_id, 0) + 1\n","\n","# 3) Identify image_ids with at least 5 captions\n","valid_ids = {img_id for img_id, cnt in caption_counts.items() if cnt >= 5}\n","\n","# 4) Print how many images meet this criterion\n","print(f\"Total images in ground-truth: {len({img['id'] for img in coco['images']})}\")\n","print(f\"Images with ≥ 5 captions: {len(valid_ids)}\")\n","\n","# 5) Filter the \"images\" list\n","filtered_images = [img for img in coco[\"images\"] if img[\"id\"] in valid_ids]\n","\n","# 6) Filter the \"annotations\" list\n","filtered_annotations = [ann for ann in coco[\"annotations\"] if ann[\"image_id\"] in valid_ids]\n","\n","# 7) Write out the filtered ground-truth JSON\n","coco_filtered = {\n","    \"images\": filtered_images,\n","    \"annotations\": filtered_annotations\n","}\n","with open(filtered_gt_path, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(coco_filtered, f, ensure_ascii=False, indent=2)\n","\n","print(f\"Filtered annotation file written to: {filtered_gt_path}\")\n","print(f\"  → Kept {len(filtered_images)} images (each ≥ 5 captions).\")\n","\n","# 8) Load and filter baseline results\n","with open(orig_baseline_path, \"r\", encoding=\"utf-8\") as f:\n","    baseline = json.load(f)\n","baseline_filtered = [r for r in baseline if r[\"image_id\"] in valid_ids]\n","with open(filtered_baseline_path, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(baseline_filtered, f, ensure_ascii=False, indent=2)\n","print(f\"Filtered baseline results written to: {filtered_baseline_path}\")\n","print(f\"  → Kept {len(baseline_filtered)} entries.\")\n","\n","# 9) Load and filter finetuned results\n","with open(orig_finetuned_path, \"r\", encoding=\"utf-8\") as f:\n","    finetuned = json.load(f)\n","finetuned_filtered = [r for r in finetuned if r[\"image_id\"] in valid_ids]\n","with open(filtered_finetuned_path, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(finetuned_filtered, f, ensure_ascii=False, indent=2)\n","print(f\"Filtered finetuned results written to: {filtered_finetuned_path}\")\n","print(f\"  → Kept {len(finetuned_filtered)} entries.\")\n","\n","# 10) Run evaluation on the filtered sets\n","print(\"\\nBaseline model evaluation (filtered):\")\n","baseline_metrics = evaluate_captions(filtered_gt_path, filtered_baseline_path)\n","print(baseline_metrics)\n","\n","print(\"\\nFinetuned model evaluation (filtered):\")\n","finetuned_metrics = evaluate_captions(filtered_gt_path, filtered_finetuned_path)\n","print(finetuned_metrics)\n","\n","# 11) Display results as tables\n","baseline_df = pd.DataFrame.from_dict(baseline_metrics, orient='index', columns=['Score'])\n","baseline_df.index.name = 'Metric'\n","print(\"\\nBaseline Model Metrics (filtered):\")\n","display(baseline_df)\n","\n","finetuned_df = pd.DataFrame.from_dict(finetuned_metrics, orient='index', columns=['Score'])\n","finetuned_df.index.name = 'Metric'\n","print(\"\\nFinetuned Model Metrics (filtered):\")\n","display(finetuned_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"7_q9_7ZUCt-1","executionInfo":{"status":"error","timestamp":1748824244817,"user_tz":420,"elapsed":78,"user":{"displayName":"Xiaofei Yan","userId":"13389607527715046645"}},"outputId":"5fa32935-6adc-4236-e7b8-619c5442eb94"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Total images in ground-truth: 87\n","Images with ≥ 5 captions: 0\n","Filtered annotation file written to: /content/drive/MyDrive/cs231n/project/eval_data/gt_at_least5.json\n","  → Kept 0 images (each ≥ 5 captions).\n","Filtered baseline results written to: /content/drive/MyDrive/cs231n/project/eval_data/baseline_at_least5.json\n","  → Kept 0 entries.\n","Filtered finetuned results written to: /content/drive/MyDrive/cs231n/project/eval_data/finetuned_at_least5.json\n","  → Kept 0 entries.\n","\n","Baseline model evaluation (filtered):\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Loading and preparing results...\n"]},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-6eebbd2d0d9a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# 10) Run evaluation on the filtered sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBaseline model evaluation (filtered):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mbaseline_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_captions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_gt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_baseline_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-0e77aed6a02f>\u001b[0m in \u001b[0;36mevaluate_captions\u001b[0;34m(annotation_file, result_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# 2) Load the generated captions (must match COCO result format)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mcocoRes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadRes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# 3) Create the COCO evaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36mloadRes\u001b[0;34m(self, resFile)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannsImgIds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannsImgIds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetImgIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                \u001b[0;34m'Results do not correspond to current coco set'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'caption'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mimgIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgIds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]}]}